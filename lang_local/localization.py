import os
import sys
import subprocess
import shutil
import math
import asyncio
import edge_tts
from concurrent.futures import ThreadPoolExecutor
from faster_whisper import WhisperModel
from moviepy import VideoFileClip, AudioFileClip, CompositeAudioClip, CompositeVideoClip, TextClip
import moviepy.video.fx as vfx
from pydub import AudioSegment


# --- 1. å­—ä½“è·¯å¾„é…ç½® (é’ˆå¯¹ Mac) ---
def get_font():
    # ä¼˜å…ˆä½¿ç”¨è‹¹æ–¹ï¼Œæ”¯æŒå¤šå›½è¯­è¨€ä¸”æ¸…æ™°
    paths = [
        "/System/Library/Fonts/Cache/PingFang.ttc",
        "/System/Library/Fonts/STHeiti Medium.ttc",
        "/Library/Fonts/Arial Unicode.ttf"
    ]
    for p in paths:
        if os.path.exists(p): return p
    return "Arial"


# --- 2. TTS åˆæˆ ---
async def generate_voice_safe(text, lang, output_path):
    voice_map = {'en': 'en-US-ChristopherNeural', 'vi': 'vi-VN-NamMinhNeural', 'zh': 'zh-CN-YunxiNeural'}
    voice = voice_map.get(lang, 'en-US-ChristopherNeural')
    try:
        communicate = edge_tts.Communicate(text, voice, rate="-5%")
        await communicate.save(output_path)
    except:
        AudioSegment.silent(duration=100).export(output_path, format="mp3")


def run_tts_worker(args):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(generate_voice_safe(*args))
    loop.close()


# --- 3. è§†é¢‘æ’ç‰ˆé€»è¾‘ ---
def adapt_shorts_layout(clip, target_size=(1080, 1920)):
    w, h = clip.size
    # åˆ‡æ‰åº•éƒ¨ 12% å»é™¤åŸå­—å¹•
    clip_no_sub = clip.cropped(y2=int(h * 0.88))
    # æ¨¡ç³ŠèƒŒæ™¯ (ç¼©æ”¾æ³•)
    bg = clip_no_sub.resized(width=100).resized(target_size).with_effects([vfx.MultiplyColor(0.4)])
    # ä¸»ç”»é¢å±…ä¸­
    main_v = clip_no_sub.resized(width=target_size[0])
    final_clip = CompositeVideoClip([bg, main_v.with_position("center")], size=target_size)
    # å¶æ•°å°ºå¯¸ä¿®å¤
    fw = target_size[0] if target_size[0] % 2 == 0 else target_size[0] - 1
    fh = target_size[1] if target_size[1] % 2 == 0 else target_size[1] - 1
    return final_clip.resized((fw, fh))


# --- 4. ä¸»æµæ°´çº¿ ---
async def process_video_pipeline(input_path, target_lang='vi'):
    base_name = os.path.splitext(os.path.basename(input_path))[0]
    temp_dir = f"temp_{base_name}"
    os.makedirs(temp_dir, exist_ok=True)

    # A. éŸ³è½¨åˆ†ç¦»
    print("ğŸš€ [1/5] åˆ†ç¦»éŸ³è½¨...")
    subprocess.run([sys.executable, "-m", "demucs.separate", "--two-stems=vocals", input_path], capture_output=True)
    vocal_wav = f"separated/htdemucs/{base_name}/vocals.wav"
    bgm_wav = f"separated/htdemucs/{base_name}/no_vocals.wav"

    # B. æé€Ÿè¯†åˆ«
    print("ğŸ™ï¸ [2/5] Faster-Whisper è¯†åˆ«ä¸ç¿»è¯‘...")
    model = WhisperModel("base", device="cpu", compute_type="int8")
    segments_gen, _ = model.transcribe(vocal_wav, task="translate")
    segments = list(segments_gen)

    # C. å¹¶è¡Œé…éŸ³
    print(f"â³ [3/5] åˆæˆé…éŸ³ ({len(segments)}æ®µ)...")
    tts_tasks = [(s.text, target_lang, f"{temp_dir}/s_{i}.mp3") for i, s in enumerate(segments)]
    with ThreadPoolExecutor(max_workers=10) as executor:
        executor.map(run_tts_worker, tts_tasks)

    # éŸ³é¢‘ç¼åˆ
    video_raw = VideoFileClip(input_path)
    full_vocal = AudioSegment.silent(duration=int(video_raw.duration * 1000))
    for i, s in enumerate(segments):
        p = f"{temp_dir}/s_{i}.mp3"
        if os.path.exists(p):
            seg_audio = AudioSegment.from_file(p)
            full_vocal = full_vocal.overlay(seg_audio[:int((s.end - s.start) * 1000)], position=int(s.start * 1000))
    vocal_final_path = f"{temp_dir}/v_final.wav"
    full_vocal.export(vocal_final_path, format="wav")

    # D. è§†é¢‘æ’ç‰ˆä¸å­—å¹•åˆ¶ä½œ
    print("ğŸ¬ [4/5] è§†é¢‘æ’ç‰ˆä¸å­—å¹•å åŠ ...")
    layout_base = adapt_shorts_layout(video_raw)

    # ç”Ÿæˆå­—å¹• Clip åˆ—è¡¨
    font_p = get_font()
    subtitle_clips = []
    for s in segments:
        duration = s.end - s.start
        if duration <= 0: continue
        txt = TextClip(
            text=s.text, font=font_p, font_size=55, color='yellow',
            stroke_color='black', stroke_width=2, method='caption',
            size=(layout_base.w * 0.85, None)
        ).with_start(s.start).with_duration(duration).with_position(('center', layout_base.h * 0.72))
        subtitle_clips.append(txt)

    # åˆæˆæœ€ç»ˆç”»é¢ (å¸ƒå±€ + å­—å¹•)
    final_video = CompositeVideoClip([layout_base] + subtitle_clips)

    # äºŒåˆ›ç‰¹æ•ˆ
    final_video = final_video.with_effects([vfx.MirrorX(), vfx.MultiplyColor(1.05)])

    # E. æ··éŸ³ä¸å¯¼å‡º
    print("ğŸ“¦ [5/5] æ··éŸ³å¹¶å¯¼å‡ºè§†é¢‘...")
    bgm = AudioFileClip(bgm_wav).with_volume_scaled(0.45)
    vocal = AudioFileClip(vocal_final_path).with_volume_scaled(2.2)
    final_video = final_video.with_audio(CompositeAudioClip([bgm, vocal]))

    # åˆ†æ®µå¯¼å‡º (æ¯ 59 ç§’ä¸€æ®µ)
    total_d = final_video.duration
    for i in range(math.ceil(total_d / 59)):
        start, end = i * 59, min((i + 1) * 59, total_d)
        final_video.subclipped(start, end).write_videofile(
            f"Final_Subbed_{base_name}_P{i + 1}.mp4",
            codec="libx264", audio_codec="aac",
            fps=24, threads=8, preset="ultrafast",
            ffmpeg_params=["-pix_fmt", "yuv420p"]
        )

    video_raw.close()
    shutil.rmtree(temp_dir)
    print("âœ… å…¨éƒ¨å®Œæˆï¼ç°åœ¨ä½ å¯ä»¥æ£€æŸ¥å¸¦å­—å¹•çš„æˆå“äº†ã€‚")


if __name__ == "__main__":
    asyncio.run(process_video_pipeline("test_video_150.mp4", "vi"))